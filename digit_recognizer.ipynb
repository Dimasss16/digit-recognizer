{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a104a680-acca-41e5-8efb-bd1e3062cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f53b7f-3927-45c7-a103-e81e0a49e413",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce0add77-a8c2-46bb-9554-3b8001788463",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"digit-recognizer/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbc12ad8-37aa-446e-b111-b40ee4bcc794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4874dcd-7c35-4365-aa1e-ba921a9068ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data)\n",
    "\n",
    "data_dev = data[0:1000].T # flip it so that each column is an example, not each row\n",
    "X_dev = data_dev[1:n]\n",
    "Y_dev = data_dev[0]\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "X_train = data_train[1:n]\n",
    "Y_train = data_train[0]\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_dev = X_dev / 255\n",
    "\n",
    "_, m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efb0bd34-5422-49a2-a345-b7bc5adec026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(m, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b28a630a-98e1-412a-93d2-2a55c74f2c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, ..., 0, 0, 0],\n",
       "       [7, 0, 0, ..., 0, 0, 0],\n",
       "       [9, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [5, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]], shape=(42000, 785))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0feffae-05d1-4522-8f47-62fd317b9a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(784, 1000))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b77016b6-2a61-468f-9d75-1c2b2dc1aba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 6, ..., 5, 0, 1], shape=(41000,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c93a075-eace-46a4-b788-22b0dcb472a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(784, 41000))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f1d19-aec3-4e70-9398-d6c3669ab126",
   "metadata": {},
   "source": [
    "# Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85f31b44-3eda-4a0f-9da9-1dd7a4387740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    W1 = np.random.randn(10, 784) * np.sqrt(2.0 / 784)  # He initialization for ReLU\n",
    "    b1 = np.zeros((10, 1))\n",
    "    W2 = np.random.randn(10, 10) * np.sqrt(2.0 / 10)    # He initialization\n",
    "    b2 = np.zeros((10, 1))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU_prime(x):\n",
    "    return x > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "\n",
    "def forward_propagation(W1, b1, W2, b2, X):\n",
    "    Z1 = W1 @ X + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2 @ A1 + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def one_hot_encode(Y):\n",
    "    one_hot = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot[np.arange(Y.size), Y] = 1\n",
    "    one_hot = one_hot.T\n",
    "    return one_hot\n",
    "\n",
    "def back_propagation(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    m = Y.size\n",
    "    one_hot = one_hot_encode(Y)\n",
    "\n",
    "    dZ2 = A2 - one_hot\n",
    "    dW2 = (1/m) * (dZ2 @ A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dZ1 = (W2.T @ dZ2) * ReLU_prime(Z1)\n",
    "    dW1 = (1/m) * (dZ1 @ X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39998d-318c-498c-994a-4f9fc1dd3cc2",
   "metadata": {},
   "source": [
    "`np.random.rand(10, 784)` returns a 2D NumPy array of shape (10, 784).\n",
    "Entries are independent Uniform(0, 1) real numbers (all non-negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec3683d-ac2d-49ef-985e-aa91370f873a",
   "metadata": {},
   "source": [
    "The only accident that might happen is over- or under-flow in the exponentials if we use softmax = exp(x)/sum(exp(x)). Overflow of a single or underflow of all elements of x will render the output more or less useless.\n",
    "\n",
    "But it is easy to guard against that by using the identity softmax(x) = softmax(x + c) which holds for any scalar c: Subtracting max(x) from x leaves a vector that has only non-positive entries, ruling out overflow and at least one element that is zero ruling out a vanishing denominator (underflow in some but not all entries is harmless)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3da8ef72-00bc-4a1b-9a41-7aab759eddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def clip_gradients(dW1, db1, dW2, db2, max_norm=1.0):\n",
    "    grad_norm = np.sqrt(np.sum(dW1**2) + np.sum(db1**2) + \n",
    "                       np.sum(dW2**2) + np.sum(db2**2))\n",
    "    \n",
    "    if grad_norm > max_norm:\n",
    "        scale = max_norm / grad_norm\n",
    "        dW1 = dW1 * scale\n",
    "        db1 = db1 * scale\n",
    "        dW2 = dW2 * scale\n",
    "        db2 = db2 * scale\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def gradient_descent(X, Y, iterations, alpha):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_propagation(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = back_propagation(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        \n",
    "        dW1, db1, dW2, db2 = clip_gradients(dW1, db1, dW2, db2, max_norm=1.0)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if (i+1) % 50 == 0:\n",
    "            acc  = get_accuracy(get_predictions(A2), Y)\n",
    "            print(f\"epoch {i+1:4d} | acc {acc:.3f}\")\n",
    "            \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fb97b60-9ac7-4e21-8368-8141388a9a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/32hfx73n42q_grncmqsc8ph80000gn/T/ipykernel_97125/3131647211.py:19: RuntimeWarning: divide by zero encountered in matmul\n",
      "  Z1 = W1 @ X + b1\n",
      "/var/folders/_1/32hfx73n42q_grncmqsc8ph80000gn/T/ipykernel_97125/3131647211.py:19: RuntimeWarning: overflow encountered in matmul\n",
      "  Z1 = W1 @ X + b1\n",
      "/var/folders/_1/32hfx73n42q_grncmqsc8ph80000gn/T/ipykernel_97125/3131647211.py:19: RuntimeWarning: invalid value encountered in matmul\n",
      "  Z1 = W1 @ X + b1\n",
      "/var/folders/_1/32hfx73n42q_grncmqsc8ph80000gn/T/ipykernel_97125/3131647211.py:21: RuntimeWarning: divide by zero encountered in matmul\n",
      "  Z2 = W2 @ A1 + b2\n",
      "/var/folders/_1/32hfx73n42q_grncmqsc8ph80000gn/T/ipykernel_97125/3131647211.py:21: RuntimeWarning: overflow encountered in matmul\n",
      "  Z2 = W2 @ A1 + b2\n",
      "/var/folders/_1/32hfx73n42q_grncmqsc8ph80000gn/T/ipykernel_97125/3131647211.py:21: RuntimeWarning: invalid value encountered in matmul\n",
      "  Z2 = W2 @ A1 + b2\n",
      "/var/folders/_1/32hfx73n42q_grncmqsc8ph80000gn/T/ipykernel_97125/3131647211.py:36: RuntimeWarning: divide by zero encountered in matmul\n",
      "  dW2 = (1/m) * (dZ2 @ A1.T)\n",
      "/var/folders/_1/32hfx73n42q_grncmqsc8ph80000gn/T/ipykernel_97125/3131647211.py:36: RuntimeWarning: overflow encountered in matmul\n",
      "  dW2 = (1/m) * (dZ2 @ A1.T)\n",
      "/var/folders/_1/32hfx73n42q_grncmqsc8ph80000gn/T/ipykernel_97125/3131647211.py:36: RuntimeWarning: invalid value encountered in matmul\n",
      "  dW2 = (1/m) * (dZ2 @ A1.T)\n",
      "/var/folders/_1/32hfx73n42q_grncmqsc8ph80000gn/T/ipykernel_97125/3131647211.py:39: RuntimeWarning: divide by zero encountered in matmul\n",
      "  dZ1 = (W2.T @ dZ2) * ReLU_prime(Z1)\n",
      "/var/folders/_1/32hfx73n42q_grncmqsc8ph80000gn/T/ipykernel_97125/3131647211.py:39: RuntimeWarning: overflow encountered in matmul\n",
      "  dZ1 = (W2.T @ dZ2) * ReLU_prime(Z1)\n",
      "/var/folders/_1/32hfx73n42q_grncmqsc8ph80000gn/T/ipykernel_97125/3131647211.py:39: RuntimeWarning: invalid value encountered in matmul\n",
      "  dZ1 = (W2.T @ dZ2) * ReLU_prime(Z1)\n",
      "/var/folders/_1/32hfx73n42q_grncmqsc8ph80000gn/T/ipykernel_97125/3131647211.py:40: RuntimeWarning: divide by zero encountered in matmul\n",
      "  dW1 = (1/m) * (dZ1 @ X.T)\n",
      "/var/folders/_1/32hfx73n42q_grncmqsc8ph80000gn/T/ipykernel_97125/3131647211.py:40: RuntimeWarning: overflow encountered in matmul\n",
      "  dW1 = (1/m) * (dZ1 @ X.T)\n",
      "/var/folders/_1/32hfx73n42q_grncmqsc8ph80000gn/T/ipykernel_97125/3131647211.py:40: RuntimeWarning: invalid value encountered in matmul\n",
      "  dW1 = (1/m) * (dZ1 @ X.T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 5 ... 4 3 5] [4 2 3 ... 4 2 3]\n",
      "epoch   50 | acc 0.368\n",
      "[4 0 3 ... 4 3 3] [4 2 3 ... 4 2 3]\n",
      "epoch  100 | acc 0.612\n",
      "[4 2 3 ... 4 2 3] [4 2 3 ... 4 2 3]\n",
      "epoch  150 | acc 0.793\n",
      "[4 2 3 ... 4 2 3] [4 2 3 ... 4 2 3]\n",
      "epoch  200 | acc 0.831\n",
      "[4 2 3 ... 4 2 3] [4 2 3 ... 4 2 3]\n",
      "epoch  250 | acc 0.852\n",
      "[4 2 3 ... 4 2 3] [4 2 3 ... 4 2 3]\n",
      "epoch  300 | acc 0.866\n",
      "[4 2 3 ... 4 2 3] [4 2 3 ... 4 2 3]\n",
      "epoch  350 | acc 0.874\n",
      "[4 2 3 ... 4 2 3] [4 2 3 ... 4 2 3]\n",
      "epoch  400 | acc 0.880\n",
      "[4 2 3 ... 4 2 3] [4 2 3 ... 4 2 3]\n",
      "epoch  450 | acc 0.885\n",
      "[4 2 3 ... 4 2 3] [4 2 3 ... 4 2 3]\n",
      "epoch  500 | acc 0.888\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 500, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d8e009-6ac5-4167-a42c-8f89a7cffdf1",
   "metadata": {},
   "source": [
    "# Now let's get rid of these warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d876f291-b233-4bc5-a231-133ee6d6cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "def safe_matmul(A, B):\n",
    "    return np.einsum('ij,jk->ik', A, B)\n",
    "\n",
    "def preprocess_sparse_data(X, min_variance=1e-8):\n",
    "    noise_scale = 1e-8\n",
    "    X_processed = X.copy()\n",
    "    \n",
    "    zero_mask = (X_processed == 0.0)\n",
    "    X_processed[zero_mask] += np.random.normal(0, noise_scale, size=np.sum(zero_mask))\n",
    "    \n",
    "    feature_vars = np.var(X_processed, axis=1)\n",
    "    low_var_mask = feature_vars < min_variance\n",
    "    \n",
    "    if np.any(low_var_mask):\n",
    "        print(f\"Adding noise to {np.sum(low_var_mask)} low-variance features\")\n",
    "        X_processed[low_var_mask] += np.random.normal(0, noise_scale, \n",
    "                                                     size=(np.sum(low_var_mask), X_processed.shape[1]))\n",
    "    \n",
    "    return X_processed\n",
    "\n",
    "def init_params():\n",
    "    W1 = np.random.randn(10, 784) * np.sqrt(2.0 / 784)\n",
    "    b1 = np.zeros((10, 1))\n",
    "    W2 = np.random.randn(10, 10) * np.sqrt(2.0 / 10)\n",
    "    b2 = np.zeros((10, 1))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU_prime(x):\n",
    "    return x > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    Z_shifted = Z - np.max(Z, axis=0, keepdims=True)\n",
    "    exp_Z = np.exp(Z_shifted)\n",
    "    return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "\n",
    "def forward_propagation_safe(W1, b1, W2, b2, X):\n",
    "    Z1 = safe_matmul(W1, X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = safe_matmul(W2, A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def one_hot_encode(Y):\n",
    "    one_hot = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot[np.arange(Y.size), Y] = 1\n",
    "    return one_hot.T\n",
    "\n",
    "def back_propagation_safe(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    m = Y.size\n",
    "    one_hot = one_hot_encode(Y)\n",
    "    \n",
    "    dZ2 = A2 - one_hot\n",
    "    dW2 = (1/m) * safe_matmul(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dZ1 = safe_matmul(W2.T, dZ2) * ReLU_prime(Z1)\n",
    "    dW1 = (1/m) * safe_matmul(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def clip_gradients(dW1, db1, dW2, db2, max_norm=1.0):\n",
    "    grad_norm = np.sqrt(np.sum(dW1**2) + np.sum(db1**2) + \n",
    "                       np.sum(dW2**2) + np.sum(db2**2))\n",
    "    \n",
    "    if grad_norm > max_norm:\n",
    "        scale = max_norm / grad_norm\n",
    "        dW1 = dW1 * scale\n",
    "        db1 = db1 * scale\n",
    "        dW2 = dW2 * scale\n",
    "        db2 = db2 * scale\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent_safe(X, Y, iterations, alpha):\n",
    "    X_processed = preprocess_sparse_data(X)\n",
    "    \n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_propagation_safe(W1, b1, W2, b2, X_processed)\n",
    "        dW1, db1, dW2, db2 = back_propagation_safe(Z1, A1, Z2, A2, W1, W2, X_processed, Y)\n",
    "        dW1, db1, dW2, db2 = clip_gradients(dW1, db1, dW2, db2, max_norm=1.0)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        \n",
    "        if (i+1) % 50 == 0:\n",
    "            acc = get_accuracy(get_predictions(A2), Y)\n",
    "            print(f\"epoch {i+1:4d} | acc {acc:.3f}\")\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cffa67b-107c-458f-bb8d-9b85818dc3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding noise to 77 low-variance features\n",
      "epoch   50 | acc 0.599\n",
      "epoch  100 | acc 0.748\n",
      "epoch  150 | acc 0.805\n",
      "epoch  200 | acc 0.840\n",
      "epoch  250 | acc 0.857\n",
      "epoch  300 | acc 0.868\n",
      "epoch  350 | acc 0.875\n",
      "epoch  400 | acc 0.880\n",
      "epoch  450 | acc 0.885\n",
      "epoch  500 | acc 0.888\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent_safe(X_train, Y_train, 500, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee7b5a0-e00c-4b3c-905f-c0866e3821f3",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "700bb831-e450-40b2-a7ea-849e959f3194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward_propagation_safe(W1, b1, W2, b2, X)\n",
    "    predictions = get_predictions(A2)\n",
    "    return predictions\n",
    "\n",
    "def test_prediction(index, W1, b1, W2, b2):\n",
    "    current_image = X_train[:, index, None]\n",
    "    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)\n",
    "    label = Y_train[index]\n",
    "    print(\"Prediction: \", prediction)\n",
    "    print(\"Label: \", label)\n",
    "    \n",
    "    current_image = current_image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37a4eb10-4eed-40de-99fd-6267d30814dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 7 9 0 0 5 6 2 5 5 5 5 7 3 1 0 2 5 3 3 0 5 5 0 1 7 4 7 7 0 6 3 9 1 6 6 1\n",
      " 0 5 3 4 2 4 3 4 8 7 7 8 4 2 3 3 9 4 1 4 6 9 5 5 7 9 0 0 5 9 4 1 9 5 4 1 0\n",
      " 5 9 1 3 0 4 8 8 5 1 4 2 6 1 2 9 0 4 5 8 1 3 8 5 5 4 4 6 7 1 7 4 2 6 4 4 4\n",
      " 0 1 7 4 5 7 5 1 0 0 7 9 1 6 0 3 6 3 8 1 9 1 8 0 4 4 2 1 2 1 7 6 2 4 0 0 3\n",
      " 0 3 0 8 7 2 9 8 6 3 2 3 0 1 7 0 2 5 7 7 1 7 0 8 8 6 6 4 0 8 0 3 7 2 1 2 6\n",
      " 9 4 5 2 2 6 8 6 9 0 4 6 1 6 1 2 2 7 6 0 7 1 1 7 8 1 0 9 9 6 9 7 4 6 8 3 9\n",
      " 2 9 6 2 9 4 0 1 9 4 1 3 3 0 2 4 1 3 5 7 2 4 3 4 0 3 0 8 8 3 8 9 0 6 9 2 4\n",
      " 0 5 5 9 2 5 0 4 6 0 4 1 2 1 2 7 9 0 7 7 6 9 4 5 4 7 2 1 1 3 9 4 6 7 9 8 3\n",
      " 9 3 0 5 8 0 1 1 3 2 4 0 4 9 0 3 4 2 4 1 1 8 8 9 2 4 4 2 7 0 9 2 7 4 1 2 1\n",
      " 2 7 6 1 3 5 8 3 0 6 7 2 0 4 5 0 9 6 4 3 1 5 0 5 8 4 2 7 4 2 4 2 8 9 2 1 1\n",
      " 4 0 5 4 9 6 5 7 6 7 3 3 8 2 4 9 3 5 0 7 0 7 7 6 7 3 4 7 0 1 7 4 5 5 6 1 8\n",
      " 6 6 6 0 1 1 0 8 0 2 8 4 2 7 1 7 0 7 4 9 9 2 2 1 4 1 8 9 4 0 9 2 6 6 1 9 2\n",
      " 4 0 2 2 0 9 9 8 1 3 2 6 1 9 2 3 2 2 2 6 5 1 4 5 6 3 5 1 0 4 6 5 9 7 7 1 2\n",
      " 0 9 4 8 4 6 3 6 7 7 9 9 2 7 2 4 9 7 6 9 7 6 8 9 7 5 9 4 1 5 2 9 8 7 6 0 2\n",
      " 4 0 3 3 6 9 9 1 8 9 9 7 7 5 1 7 1 3 4 7 7 9 5 8 3 4 4 8 9 0 4 7 9 3 4 8 3\n",
      " 7 2 2 9 8 3 4 9 3 4 9 3 1 9 0 9 1 8 6 8 0 9 2 2 0 5 8 1 7 6 9 3 6 9 8 5 7\n",
      " 1 7 0 3 7 9 3 0 3 1 8 6 1 8 6 7 9 6 7 5 0 0 1 6 2 0 5 1 4 9 2 6 6 5 3 0 1\n",
      " 7 5 5 0 4 3 2 8 6 0 2 0 1 7 2 2 3 4 4 3 8 6 0 4 5 1 3 0 6 8 3 2 6 0 9 8 8\n",
      " 6 9 1 7 8 5 6 8 0 3 0 1 4 4 3 4 9 5 4 2 0 1 5 8 1 3 8 7 6 5 6 2 9 7 8 5 2\n",
      " 7 0 4 6 4 0 1 8 8 2 3 5 5 1 9 2 0 1 3 5 1 7 4 8 1 8 4 5 0 1 8 5 0 7 9 8 2\n",
      " 0 1 3 7 2 9 0 5 6 2 6 7 6 2 7 3 7 9 2 3 1 5 3 1 9 7 0 8 8 7 4 3 4 2 3 9 6\n",
      " 9 0 3 2 8 6 2 3 1 5 8 5 3 1 3 1 8 4 2 6 3 0 7 5 4 6 4 1 8 7 8 6 6 2 2 2 9\n",
      " 1 5 2 3 8 7 2 2 9 8 8 7 9 0 7 1 9 6 4 6 2 8 0 1 9 2 3 6 6 5 7 3 6 2 4 3 6\n",
      " 3 8 1 0 2 1 8 8 1 7 3 7 4 4 1 0 8 7 5 8 5 5 8 0 9 2 6 3 7 8 8 4 8 4 0 8 9\n",
      " 2 1 0 3 7 2 5 9 2 0 1 6 9 0 1 7 6 3 3 0 6 7 4 0 8 9 1 9 1 0 7 1 8 8 7 0 4\n",
      " 6 7 3 3 6 2 5 9 1 9 1 4 0 6 3 3 3 2 9 2 7 1 6 6 7 8 9 0 9 7 5 7 3 8 1 4 9\n",
      " 3 6 8 1 6 9 8 3 1 7 5 8 6 0 0 3 1 0 0 0 4 2 5 4 7 5 6 2 8 9 1 6 6 4 7 4 4\n",
      " 9] [2 7 9 0 0 5 6 2 5 5 5 5 7 8 2 0 2 5 2 3 0 5 3 0 1 2 4 7 7 0 5 3 9 1 6 6 7\n",
      " 0 5 9 4 2 4 3 5 8 7 7 8 4 6 3 3 4 4 1 4 6 9 5 5 7 9 0 0 5 9 4 1 9 5 9 1 0\n",
      " 5 9 1 3 0 4 3 8 5 1 4 2 6 1 2 9 0 4 5 8 1 3 8 5 3 4 4 6 7 1 9 4 2 6 4 4 4\n",
      " 0 1 7 4 2 7 5 1 5 0 7 9 1 6 0 3 6 3 2 1 9 1 8 2 4 4 2 1 2 1 7 6 2 4 0 0 3\n",
      " 0 3 5 8 7 2 5 8 6 3 2 3 0 1 7 0 2 5 9 7 1 7 0 8 8 6 6 4 0 8 0 3 7 2 1 2 6\n",
      " 9 2 5 2 2 6 3 6 9 0 4 6 1 6 1 2 2 7 6 0 7 1 9 7 8 1 0 9 9 6 9 7 7 6 8 5 9\n",
      " 2 9 6 2 9 4 0 8 2 4 1 2 3 0 2 4 8 3 8 7 2 4 3 4 0 3 0 8 8 3 8 9 0 6 3 2 4\n",
      " 0 5 5 9 2 3 0 4 6 0 4 1 2 1 2 7 9 0 7 7 6 9 4 7 4 7 2 1 1 3 4 4 6 7 9 8 3\n",
      " 9 3 0 5 8 0 1 1 2 2 4 0 4 9 0 3 4 2 4 1 1 8 8 7 2 1 4 2 7 0 9 2 7 4 1 2 1\n",
      " 2 9 6 1 5 0 8 3 0 2 7 2 0 9 9 0 8 8 4 3 1 5 0 5 8 4 2 7 7 2 2 2 8 9 2 1 1\n",
      " 4 0 5 4 9 6 5 7 6 7 3 3 8 2 4 9 3 5 2 7 0 7 7 6 2 3 4 7 0 1 7 4 5 5 6 1 8\n",
      " 6 6 6 0 1 1 0 8 0 2 8 4 2 7 8 9 0 7 4 9 9 2 2 1 4 1 8 9 7 0 9 2 6 6 1 9 2\n",
      " 4 0 2 2 0 9 9 8 1 3 2 0 1 9 2 3 2 2 2 6 5 1 4 5 6 3 5 1 4 2 0 8 9 7 7 1 2\n",
      " 0 9 4 8 4 6 3 3 7 7 9 7 2 5 2 4 9 7 5 9 7 6 8 9 7 5 9 4 1 5 4 4 8 7 6 0 2\n",
      " 4 0 9 3 6 9 9 1 7 9 9 3 7 5 1 7 1 3 4 7 7 9 5 8 3 4 9 5 9 0 4 7 9 2 4 8 3\n",
      " 7 2 2 9 8 7 4 9 3 4 4 3 1 8 0 9 1 2 6 8 0 9 2 2 0 5 8 1 7 6 9 3 6 7 8 5 7\n",
      " 1 7 0 3 7 9 3 0 3 1 8 6 1 1 6 7 9 6 7 5 0 0 9 6 2 0 5 1 4 9 2 6 6 5 3 0 1\n",
      " 7 5 5 0 4 5 2 8 6 0 2 0 1 7 2 2 3 4 4 3 8 6 0 4 5 1 3 0 6 8 3 3 6 0 9 8 3\n",
      " 6 9 1 7 8 5 6 8 0 3 0 1 4 4 3 4 9 5 4 2 0 1 5 8 1 3 8 7 6 5 6 2 9 9 8 5 2\n",
      " 7 0 4 6 4 0 1 8 8 2 3 5 5 1 9 2 0 1 3 5 1 7 4 8 1 8 4 5 0 1 8 5 0 7 9 0 2\n",
      " 0 1 3 7 2 9 0 5 6 2 6 7 6 2 7 3 7 9 2 3 1 8 3 1 9 7 0 8 8 7 4 3 4 3 2 9 6\n",
      " 8 0 3 2 8 6 2 3 1 5 5 5 3 1 3 1 8 4 6 6 5 0 7 5 4 6 4 1 8 7 8 6 6 2 2 2 4\n",
      " 1 5 2 3 8 7 2 2 9 8 8 9 3 0 7 1 9 6 4 6 2 8 0 1 9 2 3 6 6 5 7 3 6 2 4 5 6\n",
      " 3 8 1 0 2 1 0 8 2 7 9 7 4 4 1 0 3 7 5 8 5 5 8 6 9 2 8 3 7 8 8 4 5 4 0 8 9\n",
      " 2 1 0 2 7 2 5 9 2 0 1 6 9 0 1 7 6 3 5 0 6 3 4 0 8 9 1 7 7 0 7 1 8 8 8 0 4\n",
      " 6 7 3 3 6 2 8 4 1 9 1 4 0 6 3 8 3 2 9 2 7 1 6 6 7 8 9 0 9 7 5 7 3 8 1 4 9\n",
      " 3 6 8 1 6 4 2 3 1 7 5 5 6 0 0 3 1 0 0 0 4 2 5 4 7 5 6 2 8 9 1 6 6 4 7 4 4\n",
      " 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.887)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_predictions = make_predictions(X_dev, W1, b1, W2, b2)\n",
    "get_accuracy(dev_predictions, Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a42ef4c6-17ce-45de-9553-6ae99da08f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [5]\n",
      "Label:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_prediction(0, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc5c9818-2be7-4789-b7ea-9e74c50f6ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 7 9 0 0 5 6 2 5 5 5 5 7 3 1 0 2 5 3 3 0 5 5 0 1 7 4 7 7 0 6 3 9 1 6 6 1\n",
      " 0 5 3 4 2 4 3 4 8 7 7 8 4 2 3 3 9 4 1 4 6 9 5 5 7 9 0 0 5 9 4 1 9 5 4 1 0\n",
      " 5 9 1 3 0 4 8 8 5 1 4 2 6 1 2 9 0 4 5 8 1 3 8 5 5 4 4 6 7 1 7 4 2 6 4 4 4\n",
      " 0 1 7 4 5 7 5 1 0 0 7 9 1 6 0 3 6 3 8 1 9 1 8 0 4 4 2 1 2 1 7 6 2 4 0 0 3\n",
      " 0 3 0 8 7 2 9 8 6 3 2 3 0 1 7 0 2 5 7 7 1 7 0 8 8 6 6 4 0 8 0 3 7 2 1 2 6\n",
      " 9 4 5 2 2 6 8 6 9 0 4 6 1 6 1 2 2 7 6 0 7 1 1 7 8 1 0 9 9 6 9 7 4 6 8 3 9\n",
      " 2 9 6 2 9 4 0 1 9 4 1 3 3 0 2 4 1 3 5 7 2 4 3 4 0 3 0 8 8 3 8 9 0 6 9 2 4\n",
      " 0 5 5 9 2 5 0 4 6 0 4 1 2 1 2 7 9 0 7 7 6 9 4 5 4 7 2 1 1 3 9 4 6 7 9 8 3\n",
      " 9 3 0 5 8 0 1 1 3 2 4 0 4 9 0 3 4 2 4 1 1 8 8 9 2 4 4 2 7 0 9 2 7 4 1 2 1\n",
      " 2 7 6 1 3 5 8 3 0 6 7 2 0 4 5 0 9 6 4 3 1 5 0 5 8 4 2 7 4 2 4 2 8 9 2 1 1\n",
      " 4 0 5 4 9 6 5 7 6 7 3 3 8 2 4 9 3 5 0 7 0 7 7 6 7 3 4 7 0 1 7 4 5 5 6 1 8\n",
      " 6 6 6 0 1 1 0 8 0 2 8 4 2 7 1 7 0 7 4 9 9 2 2 1 4 1 8 9 4 0 9 2 6 6 1 9 2\n",
      " 4 0 2 2 0 9 9 8 1 3 2 6 1 9 2 3 2 2 2 6 5 1 4 5 6 3 5 1 0 4 6 5 9 7 7 1 2\n",
      " 0 9 4 8 4 6 3 6 7 7 9 9 2 7 2 4 9 7 6 9 7 6 8 9 7 5 9 4 1 5 2 9 8 7 6 0 2\n",
      " 4 0 3 3 6 9 9 1 8 9 9 7 7 5 1 7 1 3 4 7 7 9 5 8 3 4 4 8 9 0 4 7 9 3 4 8 3\n",
      " 7 2 2 9 8 3 4 9 3 4 9 3 1 9 0 9 1 8 6 8 0 9 2 2 0 5 8 1 7 6 9 3 6 9 8 5 7\n",
      " 1 7 0 3 7 9 3 0 3 1 8 6 1 8 6 7 9 6 7 5 0 0 1 6 2 0 5 1 4 9 2 6 6 5 3 0 1\n",
      " 7 5 5 0 4 3 2 8 6 0 2 0 1 7 2 2 3 4 4 3 8 6 0 4 5 1 3 0 6 8 3 2 6 0 9 8 8\n",
      " 6 9 1 7 8 5 6 8 0 3 0 1 4 4 3 4 9 5 4 2 0 1 5 8 1 3 8 7 6 5 6 2 9 7 8 5 2\n",
      " 7 0 4 6 4 0 1 8 8 2 3 5 5 1 9 2 0 1 3 5 1 7 4 8 1 8 4 5 0 1 8 5 0 7 9 8 2\n",
      " 0 1 3 7 2 9 0 5 6 2 6 7 6 2 7 3 7 9 2 3 1 5 3 1 9 7 0 8 8 7 4 3 4 2 3 9 6\n",
      " 9 0 3 2 8 6 2 3 1 5 8 5 3 1 3 1 8 4 2 6 3 0 7 5 4 6 4 1 8 7 8 6 6 2 2 2 9\n",
      " 1 5 2 3 8 7 2 2 9 8 8 7 9 0 7 1 9 6 4 6 2 8 0 1 9 2 3 6 6 5 7 3 6 2 4 3 6\n",
      " 3 8 1 0 2 1 8 8 1 7 3 7 4 4 1 0 8 7 5 8 5 5 8 0 9 2 6 3 7 8 8 4 8 4 0 8 9\n",
      " 2 1 0 3 7 2 5 9 2 0 1 6 9 0 1 7 6 3 3 0 6 7 4 0 8 9 1 9 1 0 7 1 8 8 7 0 4\n",
      " 6 7 3 3 6 2 5 9 1 9 1 4 0 6 3 3 3 2 9 2 7 1 6 6 7 8 9 0 9 7 5 7 3 8 1 4 9\n",
      " 3 6 8 1 6 9 8 3 1 7 5 8 6 0 0 3 1 0 0 0 4 2 5 4 7 5 6 2 8 9 1 6 6 4 7 4 4\n",
      " 9] [2 7 9 0 0 5 6 2 5 5 5 5 7 8 2 0 2 5 2 3 0 5 3 0 1 2 4 7 7 0 5 3 9 1 6 6 7\n",
      " 0 5 9 4 2 4 3 5 8 7 7 8 4 6 3 3 4 4 1 4 6 9 5 5 7 9 0 0 5 9 4 1 9 5 9 1 0\n",
      " 5 9 1 3 0 4 3 8 5 1 4 2 6 1 2 9 0 4 5 8 1 3 8 5 3 4 4 6 7 1 9 4 2 6 4 4 4\n",
      " 0 1 7 4 2 7 5 1 5 0 7 9 1 6 0 3 6 3 2 1 9 1 8 2 4 4 2 1 2 1 7 6 2 4 0 0 3\n",
      " 0 3 5 8 7 2 5 8 6 3 2 3 0 1 7 0 2 5 9 7 1 7 0 8 8 6 6 4 0 8 0 3 7 2 1 2 6\n",
      " 9 2 5 2 2 6 3 6 9 0 4 6 1 6 1 2 2 7 6 0 7 1 9 7 8 1 0 9 9 6 9 7 7 6 8 5 9\n",
      " 2 9 6 2 9 4 0 8 2 4 1 2 3 0 2 4 8 3 8 7 2 4 3 4 0 3 0 8 8 3 8 9 0 6 3 2 4\n",
      " 0 5 5 9 2 3 0 4 6 0 4 1 2 1 2 7 9 0 7 7 6 9 4 7 4 7 2 1 1 3 4 4 6 7 9 8 3\n",
      " 9 3 0 5 8 0 1 1 2 2 4 0 4 9 0 3 4 2 4 1 1 8 8 7 2 1 4 2 7 0 9 2 7 4 1 2 1\n",
      " 2 9 6 1 5 0 8 3 0 2 7 2 0 9 9 0 8 8 4 3 1 5 0 5 8 4 2 7 7 2 2 2 8 9 2 1 1\n",
      " 4 0 5 4 9 6 5 7 6 7 3 3 8 2 4 9 3 5 2 7 0 7 7 6 2 3 4 7 0 1 7 4 5 5 6 1 8\n",
      " 6 6 6 0 1 1 0 8 0 2 8 4 2 7 8 9 0 7 4 9 9 2 2 1 4 1 8 9 7 0 9 2 6 6 1 9 2\n",
      " 4 0 2 2 0 9 9 8 1 3 2 0 1 9 2 3 2 2 2 6 5 1 4 5 6 3 5 1 4 2 0 8 9 7 7 1 2\n",
      " 0 9 4 8 4 6 3 3 7 7 9 7 2 5 2 4 9 7 5 9 7 6 8 9 7 5 9 4 1 5 4 4 8 7 6 0 2\n",
      " 4 0 9 3 6 9 9 1 7 9 9 3 7 5 1 7 1 3 4 7 7 9 5 8 3 4 9 5 9 0 4 7 9 2 4 8 3\n",
      " 7 2 2 9 8 7 4 9 3 4 4 3 1 8 0 9 1 2 6 8 0 9 2 2 0 5 8 1 7 6 9 3 6 7 8 5 7\n",
      " 1 7 0 3 7 9 3 0 3 1 8 6 1 1 6 7 9 6 7 5 0 0 9 6 2 0 5 1 4 9 2 6 6 5 3 0 1\n",
      " 7 5 5 0 4 5 2 8 6 0 2 0 1 7 2 2 3 4 4 3 8 6 0 4 5 1 3 0 6 8 3 3 6 0 9 8 3\n",
      " 6 9 1 7 8 5 6 8 0 3 0 1 4 4 3 4 9 5 4 2 0 1 5 8 1 3 8 7 6 5 6 2 9 9 8 5 2\n",
      " 7 0 4 6 4 0 1 8 8 2 3 5 5 1 9 2 0 1 3 5 1 7 4 8 1 8 4 5 0 1 8 5 0 7 9 0 2\n",
      " 0 1 3 7 2 9 0 5 6 2 6 7 6 2 7 3 7 9 2 3 1 8 3 1 9 7 0 8 8 7 4 3 4 3 2 9 6\n",
      " 8 0 3 2 8 6 2 3 1 5 5 5 3 1 3 1 8 4 6 6 5 0 7 5 4 6 4 1 8 7 8 6 6 2 2 2 4\n",
      " 1 5 2 3 8 7 2 2 9 8 8 9 3 0 7 1 9 6 4 6 2 8 0 1 9 2 3 6 6 5 7 3 6 2 4 5 6\n",
      " 3 8 1 0 2 1 0 8 2 7 9 7 4 4 1 0 3 7 5 8 5 5 8 6 9 2 8 3 7 8 8 4 5 4 0 8 9\n",
      " 2 1 0 2 7 2 5 9 2 0 1 6 9 0 1 7 6 3 5 0 6 3 4 0 8 9 1 7 7 0 7 1 8 8 8 0 4\n",
      " 6 7 3 3 6 2 8 4 1 9 1 4 0 6 3 8 3 2 9 2 7 1 6 6 7 8 9 0 9 7 5 7 3 8 1 4 9\n",
      " 3 6 8 1 6 4 2 3 1 7 5 5 6 0 0 3 1 0 0 0 4 2 5 4 7 5 6 2 8 9 1 6 6 4 7 4 4\n",
      " 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.887)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_predictions = make_predictions(X_dev, W1, b1, W2, b2)\n",
    "get_accuracy(dev_predictions, Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f06dde-f261-429d-a68a-fd7f04a5abc6",
   "metadata": {},
   "source": [
    "This is just slightly lower than our training accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860e17d-ea10-46ea-ab79-49b40cf0a3e1",
   "metadata": {},
   "source": [
    "# Trying two layers with 16 activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad81c9dd-e660-436b-8130-95a345bab124",
   "metadata": {},
   "source": [
    "We'll reintroduce some functions here so that this part is runnable separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0f8e354-9999-4569-95ad-3dd0e5893f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e07f1db-4d0d-4891-9897-06740a2977b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params_2hidden():\n",
    "    # First hidden layer: 784 -> 16\n",
    "    W1 = np.random.randn(16, 784) * np.sqrt(2.0 / 784)\n",
    "    b1 = np.zeros((16, 1))\n",
    "    \n",
    "    # Second hidden layer: 16 -> 16  \n",
    "    W2 = np.random.randn(16, 16) * np.sqrt(2.0 / 16)\n",
    "    b2 = np.zeros((16, 1))\n",
    "    \n",
    "    # Output layer: 16 -> 10\n",
    "    W3 = np.random.randn(10, 16) * np.sqrt(2.0 / 16)\n",
    "    b3 = np.zeros((10, 1))\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07436842-347b-4b6d-86c2-426c44ea476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU_prime(x):\n",
    "    return x > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    Z_shifted = Z - np.max(Z, axis=0, keepdims=True)\n",
    "    exp_Z = np.exp(Z_shifted)\n",
    "    return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65bad4ee-af93-4faf-ac96-20c3b7380707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_2hidden(W1, b1, W2, b2, W3, b3, X):\n",
    "    # First hidden layer\n",
    "    Z1 = safe_matmul(W1, X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    \n",
    "    # Second hidden layer\n",
    "    Z2 = safe_matmul(W2, A1) + b2\n",
    "    A2 = ReLU(Z2)\n",
    "    \n",
    "    # Output layer\n",
    "    Z3 = safe_matmul(W3, A2) + b3\n",
    "    A3 = softmax(Z3)\n",
    "    \n",
    "    return Z1, A1, Z2, A2, Z3, A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f731393f-28be-4fd1-addb-fac32e62fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation_2hidden(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, Y):\n",
    "    m = Y.size\n",
    "    one_hot = one_hot_encode(Y)\n",
    "    \n",
    "    # Output layer gradients\n",
    "    dZ3 = A3 - one_hot\n",
    "    dW3 = (1/m) * safe_matmul(dZ3, A2.T)\n",
    "    db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    # Second hidden layer gradients\n",
    "    dZ2 = safe_matmul(W3.T, dZ3) * ReLU_prime(Z2)\n",
    "    dW2 = (1/m) * safe_matmul(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    # First hidden layer gradients\n",
    "    dZ1 = safe_matmul(W2.T, dZ2) * ReLU_prime(Z1)\n",
    "    dW1 = (1/m) * safe_matmul(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    return dW1, db1, dW2, db2, dW3, db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "320e3f4c-8476-47e1-8b79-31f174e9eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params_2hidden(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "    W3 = W3 - alpha * dW3\n",
    "    b3 = b3 - alpha * db3\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "611567d8-33f5-4bba-9622-c9e17de8cef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients_2hidden(dW1, db1, dW2, db2, dW3, db3, max_norm=1.0):\n",
    "    grad_norm = np.sqrt(np.sum(dW1**2) + np.sum(db1**2) + \n",
    "                       np.sum(dW2**2) + np.sum(db2**2) +\n",
    "                       np.sum(dW3**2) + np.sum(db3**2))\n",
    "    \n",
    "    if grad_norm > max_norm:\n",
    "        scale = max_norm / grad_norm\n",
    "        dW1 = dW1 * scale\n",
    "        db1 = db1 * scale\n",
    "        dW2 = dW2 * scale\n",
    "        db2 = db2 * scale\n",
    "        dW3 = dW3 * scale\n",
    "        db3 = db3 * scale\n",
    "    \n",
    "    return dW1, db1, dW2, db2, dW3, db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09a6147b-24e4-4c30-9276-bacd84fbfeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_2hidden(A3):\n",
    "    return np.argmax(A3, 0)\n",
    "\n",
    "def get_accuracy_2hidden(predictions, Y):\n",
    "    return np.sum(predictions == Y) / Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28d3bcef-34ce-4143-9678-87aca141fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_2hidden(X, Y, iterations, alpha):\n",
    "    print(\"Training Two Hidden Layers Network\")\n",
    "    print(\"Architecture: 784 -> 16 -> 16 -> 10\")\n",
    "    \n",
    "    total_params = (784*16 + 16) + (16*16 + 16) + (16*10 + 10)\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    \n",
    "    X_processed = preprocess_sparse_data(X)\n",
    "    \n",
    "    W1, b1, W2, b2, W3, b3 = init_params_2hidden()\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # Forward pass\n",
    "        Z1, A1, Z2, A2, Z3, A3 = forward_propagation_2hidden(W1, b1, W2, b2, W3, b3, X_processed)\n",
    "        \n",
    "        # Backward pass\n",
    "        dW1, db1, dW2, db2, dW3, db3 = back_propagation_2hidden(Z1, A1, Z2, A2, Z3, A3, \n",
    "                                                                W1, W2, W3, X_processed, Y)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        dW1, db1, dW2, db2, dW3, db3 = clip_gradients_2hidden(dW1, db1, dW2, db2, dW3, db3, \n",
    "                                                              max_norm=1.0)\n",
    "        \n",
    "        # Update parameters\n",
    "        W1, b1, W2, b2, W3, b3 = update_params_2hidden(W1, b1, W2, b2, W3, b3, \n",
    "                                                       dW1, db1, dW2, db2, dW3, db3, alpha)\n",
    "        \n",
    "        # Print progress\n",
    "        if (i+1) % 50 == 0:\n",
    "            acc = get_accuracy_2hidden(get_predictions_2hidden(A3), Y)\n",
    "            print(f\"epoch {i+1:4d} | acc {acc:.3f}\")\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed499769-28f0-4432-b251-8936dc9dbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_2hidden(X, W1, b1, W2, b2, W3, b3):\n",
    "    _, _, _, _, _, A3 = forward_propagation_2hidden(W1, b1, W2, b2, W3, b3, X)\n",
    "    predictions = get_predictions_2hidden(A3)\n",
    "    return predictions\n",
    "\n",
    "def test_prediction_2hidden(index, W1, b1, W2, b2, W3, b3):\n",
    "    current_image = X_train[:, index, None]\n",
    "    prediction = make_predictions_2hidden(X_train[:, index, None], W1, b1, W2, b2, W3, b3)\n",
    "    label = Y_train[index]\n",
    "    \n",
    "    print(\"Prediction: \", prediction)\n",
    "    print(\"Label: \", label)\n",
    "    \n",
    "    current_image = current_image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69acf3a7-06ed-46dd-96bb-a2db97386765",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43919e0e-99d0-4def-a345-ca2e773e6526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Two Hidden Layers Network\n",
      "Architecture: 784 -> 16 -> 16 -> 10\n",
      "Total parameters: 13002\n",
      "Adding noise to 77 low-variance features\n",
      "epoch   50 | acc 0.469\n",
      "epoch  100 | acc 0.725\n",
      "epoch  150 | acc 0.808\n",
      "epoch  200 | acc 0.841\n",
      "epoch  250 | acc 0.860\n",
      "epoch  300 | acc 0.873\n",
      "epoch  350 | acc 0.882\n",
      "epoch  400 | acc 0.888\n",
      "epoch  450 | acc 0.893\n",
      "epoch  500 | acc 0.898\n"
     ]
    }
   ],
   "source": [
    "W1_2h, b1_2h, W2_2h, b2_2h, W3_2h, b3_2h = gradient_descent_2hidden(X_train, Y_train, 500, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b33e101-f583-4b64-abbe-d1dbf995bb3c",
   "metadata": {},
   "source": [
    "# Testing on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b40b6399-3fbc-4cb6-b812-839ac775657b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two hidden layers network dev acc: 0.897 (89.7%)\n"
     ]
    }
   ],
   "source": [
    "dev_predictions_2h = make_predictions_2hidden(X_dev, W1_2h, b1_2h, W2_2h, b2_2h, W3_2h, b3_2h)\n",
    "dev_accuracy_2h = get_accuracy_2hidden(dev_predictions_2h, Y_dev)\n",
    "\n",
    "print(f\"Two hidden layers network dev acc: {dev_accuracy_2h:.3f} ({dev_accuracy_2h*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37e1fb24-06fd-4f6d-8135-f556f213130c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 7 9 0 0 5 6 2 5 5 5 5 7 3 1 0 2 5 3 3 0 5 5 0 1 7 4 7 7 0 6 3 9 1 6 6 1\n",
      " 0 5 3 4 2 4 3 4 8 7 7 8 4 2 3 3 9 4 1 4 6 9 5 5 7 9 0 0 5 9 4 1 9 5 4 1 0\n",
      " 5 9 1 3 0 4 8 8 5 1 4 2 6 1 2 9 0 4 5 8 1 3 8 5 5 4 4 6 7 1 7 4 2 6 4 4 4\n",
      " 0 1 7 4 5 7 5 1 0 0 7 9 1 6 0 3 6 3 8 1 9 1 8 0 4 4 2 1 2 1 7 6 2 4 0 0 3\n",
      " 0 3 0 8 7 2 9 8 6 3 2 3 0 1 7 0 2 5 7 7 1 7 0 8 8 6 6 4 0 8 0 3 7 2 1 2 6\n",
      " 9 4 5 2 2 6 8 6 9 0 4 6 1 6 1 2 2 7 6 0 7 1 1 7 8 1 0 9 9 6 9 7 4 6 8 3 9\n",
      " 2 9 6 2 9 4 0 1 9 4 1 3 3 0 2 4 1 3 5 7 2 4 3 4 0 3 0 8 8 3 8 9 0 6 9 2 4\n",
      " 0 5 5 9 2 5 0 4 6 0 4 1 2 1 2 7 9 0 7 7 6 9 4 5 4 7 2 1 1 3 9 4 6 7 9 8 3\n",
      " 9 3 0 5 8 0 1 1 3 2 4 0 4 9 0 3 4 2 4 1 1 8 8 9 2 4 4 2 7 0 9 2 7 4 1 2 1\n",
      " 2 7 6 1 3 5 8 3 0 6 7 2 0 4 5 0 9 6 4 3 1 5 0 5 8 4 2 7 4 2 4 2 8 9 2 1 1\n",
      " 4 0 5 4 9 6 5 7 6 7 3 3 8 2 4 9 3 5 0 7 0 7 7 6 7 3 4 7 0 1 7 4 5 5 6 1 8\n",
      " 6 6 6 0 1 1 0 8 0 2 8 4 2 7 1 7 0 7 4 9 9 2 2 1 4 1 8 9 4 0 9 2 6 6 1 9 2\n",
      " 4 0 2 2 0 9 9 8 1 3 2 6 1 9 2 3 2 2 2 6 5 1 4 5 6 3 5 1 0 4 6 5 9 7 7 1 2\n",
      " 0 9 4 8 4 6 3 6 7 7 9 9 2 7 2 4 9 7 6 9 7 6 8 9 7 5 9 4 1 5 2 9 8 7 6 0 2\n",
      " 4 0 3 3 6 9 9 1 8 9 9 7 7 5 1 7 1 3 4 7 7 9 5 8 3 4 4 8 9 0 4 7 9 3 4 8 3\n",
      " 7 2 2 9 8 3 4 9 3 4 9 3 1 9 0 9 1 8 6 8 0 9 2 2 0 5 8 1 7 6 9 3 6 9 8 5 7\n",
      " 1 7 0 3 7 9 3 0 3 1 8 6 1 8 6 7 9 6 7 5 0 0 1 6 2 0 5 1 4 9 2 6 6 5 3 0 1\n",
      " 7 5 5 0 4 3 2 8 6 0 2 0 1 7 2 2 3 4 4 3 8 6 0 4 5 1 3 0 6 8 3 2 6 0 9 8 8\n",
      " 6 9 1 7 8 5 6 8 0 3 0 1 4 4 3 4 9 5 4 2 0 1 5 8 1 3 8 7 6 5 6 2 9 7 8 5 2\n",
      " 7 0 4 6 4 0 1 8 8 2 3 5 5 1 9 2 0 1 3 5 1 7 4 8 1 8 4 5 0 1 8 5 0 7 9 8 2\n",
      " 0 1 3 7 2 9 0 5 6 2 6 7 6 2 7 3 7 9 2 3 1 5 3 1 9 7 0 8 8 7 4 3 4 2 3 9 6\n",
      " 9 0 3 2 8 6 2 3 1 5 8 5 3 1 3 1 8 4 2 6 3 0 7 5 4 6 4 1 8 7 8 6 6 2 2 2 9\n",
      " 1 5 2 3 8 7 2 2 9 8 8 7 9 0 7 1 9 6 4 6 2 8 0 1 9 2 3 6 6 5 7 3 6 2 4 3 6\n",
      " 3 8 1 0 2 1 8 8 1 7 3 7 4 4 1 0 8 7 5 8 5 5 8 0 9 2 6 3 7 8 8 4 8 4 0 8 9\n",
      " 2 1 0 3 7 2 5 9 2 0 1 6 9 0 1 7 6 3 3 0 6 7 4 0 8 9 1 9 1 0 7 1 8 8 7 0 4\n",
      " 6 7 3 3 6 2 5 9 1 9 1 4 0 6 3 3 3 2 9 2 7 1 6 6 7 8 9 0 9 7 5 7 3 8 1 4 9\n",
      " 3 6 8 1 6 9 8 3 1 7 5 8 6 0 0 3 1 0 0 0 4 2 5 4 7 5 6 2 8 9 1 6 6 4 7 4 4\n",
      " 9] [2 7 9 0 0 5 6 2 5 5 5 5 7 8 2 0 2 5 2 3 0 5 3 0 1 2 4 7 7 0 5 3 9 1 6 6 7\n",
      " 0 5 9 4 2 4 3 5 8 7 7 8 4 6 3 3 4 4 1 4 6 9 5 5 7 9 0 0 5 9 4 1 9 5 9 1 0\n",
      " 5 9 1 3 0 4 3 8 5 1 4 2 6 1 2 9 0 4 5 8 1 3 8 5 3 4 4 6 7 1 9 4 2 6 4 4 4\n",
      " 0 1 7 4 2 7 5 1 5 0 7 9 1 6 0 3 6 3 2 1 9 1 8 2 4 4 2 1 2 1 7 6 2 4 0 0 3\n",
      " 0 3 5 8 7 2 5 8 6 3 2 3 0 1 7 0 2 5 9 7 1 7 0 8 8 6 6 4 0 8 0 3 7 2 1 2 6\n",
      " 9 2 5 2 2 6 3 6 9 0 4 6 1 6 1 2 2 7 6 0 7 1 9 7 8 1 0 9 9 6 9 7 7 6 8 5 9\n",
      " 2 9 6 2 9 4 0 8 2 4 1 2 3 0 2 4 8 3 8 7 2 4 3 4 0 3 0 8 8 3 8 9 0 6 3 2 4\n",
      " 0 5 5 9 2 3 0 4 6 0 4 1 2 1 2 7 9 0 7 7 6 9 4 7 4 7 2 1 1 3 4 4 6 7 9 8 3\n",
      " 9 3 0 5 8 0 1 1 2 2 4 0 4 9 0 3 4 2 4 1 1 8 8 7 2 1 4 2 7 0 9 2 7 4 1 2 1\n",
      " 2 9 6 1 5 0 8 3 0 2 7 2 0 9 9 0 8 8 4 3 1 5 0 5 8 4 2 7 7 2 2 2 8 9 2 1 1\n",
      " 4 0 5 4 9 6 5 7 6 7 3 3 8 2 4 9 3 5 2 7 0 7 7 6 2 3 4 7 0 1 7 4 5 5 6 1 8\n",
      " 6 6 6 0 1 1 0 8 0 2 8 4 2 7 8 9 0 7 4 9 9 2 2 1 4 1 8 9 7 0 9 2 6 6 1 9 2\n",
      " 4 0 2 2 0 9 9 8 1 3 2 0 1 9 2 3 2 2 2 6 5 1 4 5 6 3 5 1 4 2 0 8 9 7 7 1 2\n",
      " 0 9 4 8 4 6 3 3 7 7 9 7 2 5 2 4 9 7 5 9 7 6 8 9 7 5 9 4 1 5 4 4 8 7 6 0 2\n",
      " 4 0 9 3 6 9 9 1 7 9 9 3 7 5 1 7 1 3 4 7 7 9 5 8 3 4 9 5 9 0 4 7 9 2 4 8 3\n",
      " 7 2 2 9 8 7 4 9 3 4 4 3 1 8 0 9 1 2 6 8 0 9 2 2 0 5 8 1 7 6 9 3 6 7 8 5 7\n",
      " 1 7 0 3 7 9 3 0 3 1 8 6 1 1 6 7 9 6 7 5 0 0 9 6 2 0 5 1 4 9 2 6 6 5 3 0 1\n",
      " 7 5 5 0 4 5 2 8 6 0 2 0 1 7 2 2 3 4 4 3 8 6 0 4 5 1 3 0 6 8 3 3 6 0 9 8 3\n",
      " 6 9 1 7 8 5 6 8 0 3 0 1 4 4 3 4 9 5 4 2 0 1 5 8 1 3 8 7 6 5 6 2 9 9 8 5 2\n",
      " 7 0 4 6 4 0 1 8 8 2 3 5 5 1 9 2 0 1 3 5 1 7 4 8 1 8 4 5 0 1 8 5 0 7 9 0 2\n",
      " 0 1 3 7 2 9 0 5 6 2 6 7 6 2 7 3 7 9 2 3 1 8 3 1 9 7 0 8 8 7 4 3 4 3 2 9 6\n",
      " 8 0 3 2 8 6 2 3 1 5 5 5 3 1 3 1 8 4 6 6 5 0 7 5 4 6 4 1 8 7 8 6 6 2 2 2 4\n",
      " 1 5 2 3 8 7 2 2 9 8 8 9 3 0 7 1 9 6 4 6 2 8 0 1 9 2 3 6 6 5 7 3 6 2 4 5 6\n",
      " 3 8 1 0 2 1 0 8 2 7 9 7 4 4 1 0 3 7 5 8 5 5 8 6 9 2 8 3 7 8 8 4 5 4 0 8 9\n",
      " 2 1 0 2 7 2 5 9 2 0 1 6 9 0 1 7 6 3 5 0 6 3 4 0 8 9 1 7 7 0 7 1 8 8 8 0 4\n",
      " 6 7 3 3 6 2 8 4 1 9 1 4 0 6 3 8 3 2 9 2 7 1 6 6 7 8 9 0 9 7 5 7 3 8 1 4 9\n",
      " 3 6 8 1 6 4 2 3 1 7 5 5 6 0 0 3 1 0 0 0 4 2 5 4 7 5 6 2 8 9 1 6 6 4 7 4 4\n",
      " 9]\n",
      "\n",
      "Comparison:\n",
      "Single Hidden Layer (10): 0.887 (88.7%)\n",
      "Two Hidden Layers (16-16): 0.897 (89.7%)\n",
      "Improvement: +1.0 percentage points\n"
     ]
    }
   ],
   "source": [
    "dev_predictions_1h = make_predictions(X_dev, W1, b1, W2, b2)\n",
    "dev_accuracy_1h = get_accuracy(dev_predictions_1h, Y_dev)\n",
    "\n",
    "print(f\"Comparison:\")\n",
    "print(f\"Single Hidden Layer (10): {dev_accuracy_1h:.3f} ({dev_accuracy_1h*100:.1f}%)\")\n",
    "print(f\"Two Hidden Layers (16-16): {dev_accuracy_2h:.3f} ({dev_accuracy_2h*100:.1f}%)\")\n",
    "\n",
    "improvement = (dev_accuracy_2h - dev_accuracy_1h) * 100\n",
    "print(f\"Improvement: {improvement:+.1f} percentage points\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16018210-463c-43ac-bb98-f16ef9d9fa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [8]\n",
      "Label:  8\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_prediction_2hidden(20, W1_2h, b1_2h, W2_2h, b2_2h, W3_2h, b3_2h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256e54a-64c9-4315-814c-ec390f5d60f8",
   "metadata": {},
   "source": [
    "# Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ace1bd-c01f-4c70-8be6-c445fc45a772",
   "metadata": {},
   "source": [
    "We built a NumPy-only digit recognizer, starting simple and then adding a second hidden layer. The biggest early failure was that the model was essentially randomly guessing with accuracy of 11%. That was primarity due to a wrong softmax (computed across the whole matrix instead of per example), risky weight/bias initialization, incorrect bias-gradient shapes, and inconsistent normalization between train and dev. We then implemented the following fixes: a numerically stable, per-example softmax; *He* initialization with zero biases; proper `axis`/`keepdims` in gradient sums; and identical preprocessing for train/dev.\n",
    "\n",
    "Once learning started, training threw some numerical warnings caused by MNIST’s sparse pixels and some zero-variance features. We stabilized this by \n",
    "\n",
    "(1) switching to a “safe” matmul path (using `einsum`/`tensordot`), \n",
    "\n",
    "(2) adding tiny noise to exact zeros, and \n",
    "\n",
    "(3) clipping gradient norms.\n",
    "\n",
    "With those fixes in place, both the single-layer and the two-layer networks trained cleanly. The deeper model performed slightly better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter_venv)",
   "language": "python",
   "name": "jupyter_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
